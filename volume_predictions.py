# -*- coding: utf-8 -*-
"""Volume Predictions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TQDcBl-HIzKzcsZv_R1i9hvQVR5ugnpG

# Volume Predictions
## Predicting Volume Based on Feature Variables  

### Objective:  
Any  fast-moving consumer goods (FMCG) industry faces challenges in accurately predicting daily sales volume due to the influence of various external factors like weather conditions and holidays. Inaccurate volume predictions can lead to overstocking or understocking, resulting in financial losses and reduced customer satisfaction. Therefore, there is a need for a robust and accurate forecasting model that can effectively predict daily sales volume by considering these external factors, enabling businesses to optimize inventory management and improve overall operational efficiency.

# Importing Dataframe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('data volume.csv', index_col='Date',parse_dates=True)
df.tail(20)

"""# EDA on Features

"""

df = df.dropna()

df['Year'] = df.index.year
df['Month'] = df.index.month
df['Day'] = df.index.day
df['Quater'] = df.index.quarter
df['Day of Week'] = df.index.dayofweek
# Creating different columns for year, month and days to better analysis of the data

df.describe()

df.info()

df['Volume'].plot(figsize=(20,5))
plt.grid(True)
plt.ylabel('Volume')

# Annual Sales Volume
yearly_volume = df.groupby('Year')['Volume'].sum().plot(kind='bar', figsize=(12, 6))
plt.title('Total Volume per Year'); plt.xlabel('Year'); plt.ylabel('Volume')
plt.grid(True)
plt.show()

# Volume through the months in years
fig, axes = plt.subplots(3, 3, figsize=(15, 12), sharey=True)
fig.suptitle('Monthly Volume Breakdown', fontsize=16)
plt.subplots_adjust(hspace=0.5, wspace=0.3)

for i, year in enumerate(df['Year'].unique()):
    if i < 9:
        df[df['Year'] == year].groupby('Month')['Volume'].sum().plot(kind='bar', ax=axes[i // 3, i % 3])
        axes[i // 3, i % 3].set_title(year)
        axes[i // 3, i % 3].set_xlabel('Month')
    else:
        break

for i in range(len(df['Year'].unique()), 9):
    axes[i // 3, i % 3].axis('off')

# Quaterly Sales Volume
quarterly_volume = df.groupby('Quater')['Volume'].sum()

plt.figure(figsize=(10, 6))
plt.bar(quarterly_volume.index, quarterly_volume.values)
plt.title('Total Volume per Quarter')
plt.xlabel('Quarter')
plt.ylabel('Volume')
plt.grid(True)
plt.show()

# Daily Sales Volume
daily_volume = df.groupby('Day of Week')['Volume'].sum()

plt.figure(figsize=(10, 6))
plt.bar(daily_volume.index, daily_volume.values)
plt.title('Total Volume per Day of Week')
plt.xlabel('Day of Week (0=Monday, 6=Sunday)')
plt.ylabel('Volume')
plt.grid(True)
plt.show()

# Distribution and density of Volume
fig, axes = plt.subplots(1, 2, figsize=(17,5))
plt.grid(True)

df['Volume'].hist(ax=axes[0])
axes[0].set_title('Distribution of Volume')
df['Volume'].plot(kind='kde', ax=axes[1])
axes[1].set_title('Density Plot of Volume')

plt.show()

# Distribution and density of Avg. Temp.
fig, axes = plt.subplots(1, 2, figsize=(12,6))

df['Avg. Temp.'].hist(ax=axes[0])
axes[0].set_title('Distr bution of Avg. Temp.')
df['Avg. Temp.'].plot(kind='kde', ax=axes[1])
axes[1].set_title('Density Plot of Avg. Temp.')

plt.show()

# Distribution and density of Max_Temp
fig, axes = plt.subplots(1, 2, figsize=(12,6))

df['Max_Temp'].hist(ax=axes[0])
axes[0].set_title('Distr bution of Max_Temp')
df['Max_Temp'].plot(kind='kde', ax=axes[1])
axes[1].set_title('Density Plot of Max_Temp')

plt.show()

# Distribution and density of Min._Temp.
fig, axes = plt.subplots(1, 2, figsize=(12,6))

df['Min._Temp.'].hist(ax=axes[0])
axes[0].set_title('Distr bution of Min._Temp.')
df['Min._Temp.'].plot(kind='kde', ax=axes[1])
axes[1].set_title('Density Plot of AMin._Temp.')

plt.show()

#Yearly Sales Volume Distribution

plt.figure(figsize=(20,15))
df.boxplot(column='Volume', by='Month')
plt.xticks(rotation=45)
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(15,5))

axes[0].scatter(df['Avg. Temp.'], df['Volume']) # Access the first Axes object using axes[0]
axes[0].set_title('Scatter Plot: Volume vs. Avg. Temp.')
axes[0].set_xlabel('Avg. Temp.')
axes[0].set_ylabel('Volume')
axes[0].grid(True)

axes[1].scatter(df['Percipitation'], df['Volume']) # Access the second Axes object using axes[1]
axes[1].set_title('Scatter Plot: Volume vs. Percipitation')
axes[1].set_xlabel('Precipitation')
axes[1].set_ylabel('Volume')
axes[1].grid(True)


plt.show()

# Correlation Matrix of Sales Data

plt.figure(figsize=(15,6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Volume Distribution: Holidays vs. Non-Holidays
holiday_volume = df[df['Holiday'] == 1]['Volume'].sum()
non_holiday_volume = df[df['Holiday'] == 0]['Volume'].sum()

labels = ['Holidays', 'Non-Holidays']
sizes = [holiday_volume, non_holiday_volume]

plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)
plt.title('Volume Distribution: Holidays vs. Non-Holidays')
plt.axis('equal')
print("Total Volume on Holidays:", holiday_volume)
print("Total Volume on Non-Holidays:", non_holiday_volume)

# Sales Volume Distribution: Holidays vs. Non-Holidays

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
df[df['Holiday'] == 1]['Volume'].hist()
plt.title('Distribution of Volume on Holidays')
plt.subplot(1, 2, 2)
df[df['Holiday'] == 0]['Volume'].hist()
plt.title('Distribution of Volume on Non-Holidays')
plt.tight_layout()
plt.show()

# Sales Volume on Holidays vs. Non-Holidays

plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)
df[df['Holiday'] == 1]['Volume'].plot()
plt.title('Volume on Holidays')
plt.subplot(2, 1, 2)

df[df['Holiday'] == 0]['Volume'].plot()
plt.title('Volume on Non-Holidays')
plt.tight_layout()
plt.show()

# Sales Volume on Holidays vs. Non-Holidays
from statsmodels.tsa.holtwinters import SimpleExpSmoothing

plt.figure(figsize=(12, 6))
plt.subplot(2, 1, 1)
df[df['Holiday'] == 1]['Volume'].plot()
plt.title('Volume on Holidays')
plt.subplot(2, 1, 2)

model = SimpleExpSmoothing(df[df['Holiday'] == 0]['Volume']).fit(smoothing_level=0.1, optimized=False)
smoothed_values = model.fittedvalues
plt.plot(smoothed_values, label='Smoothed Volume')
plt.title('Volume on Non-Holidays')
plt.tight_layout()
plt.show()

"""# ADF test

The augmented Dickey-Fuller (ADF) test is a statistical test that determines if a time series has a unit root. It's used to determine if a series is stationary or not.
"""

df_0 = df[['Volume' , 'Avg. Temp.' , 'Percipitation','Holiday']]
df_0

from statsmodels.tsa.stattools import adfuller

## Adf Test Function

def adf_test(series,title=''):
    """
    Pass in a time series and an optional title, returns an ADF report
    """
    print(f'Augmented Dickey-Fuller Test: {title}')
    result = adfuller(series.dropna(),autolag='AIC')

    labels = ['ADF test statistic','p-value','# lags used','# observations']
    out = pd.Series(result[0:4],index=labels)

    for key,val in result[4].items():
        out[f'critical value ({key})']=val

    print(out.to_string())

    if result[1] <= 0.05:
        print("Strong evidence against the null hypothesis")
        print("Reject the null hypothesis")
        print("Data has no unit root and is stationary")
    else:
        print("Weak evidence against the null hypothesis")
        print("Fail to reject the null hypothesis")
        print("Data has a unit root and is non-stationary")

adf_test(df_0['Volume'],title='Volume')
print()
adf_test(df_0['Avg. Temp.'],title='Avg. Temp.')
print()
adf_test(df_0['Percipitation'],title='Percipitation')
print()
adf_test(df_0['Holiday'],title='Holiday')

df_transformed = df_0[['Volume' , 'Avg. Temp.' , 'Percipitation']].diff().dropna()

df_transformed['Holiday'] = df_0['Holiday'].values[1:len(df_transformed)+1]
df_transformed=df_transformed.dropna()
df_transformed

df_transformed

adf_test(df_transformed['Volume'],title='Volume')
print()
adf_test(df_transformed['Avg. Temp.'],title='Avg. Temp.')
print()
adf_test(df_transformed['Percipitation'],title='Percipitation')
print()
adf_test(df_transformed['Holiday'],title='Holiday')

"""# Train Test Split"""

train=df.iloc[:-36]
test=df.iloc[-36:]
len(train), len(test)

train_diff=df_transformed.iloc[:-36]
test_diff=df_transformed.iloc[-36:]
len(train), len(test)

"""# Model Building

# SARIMAX
"""

!pip install pmdarima
from pmdarima import auto_arima
from statsmodels.tsa.statespace.sarimax import SARIMAX

#auto_arima(df_transformed['Volume'], seasonal=True, m=365).summary()

model = SARIMAX(train_diff['Volume'],exog=train_diff[['Holiday', 'Avg. Temp.', 'Percipitation']], order=(5,0,1), enforce_invertibility=False)
result = model.fit()
result.summary()

start=len(train_diff)
end=len(train_diff)+len(test_diff)-1
pred=result.predict(start=start,end=end,exog=test_diff[['Holiday', 'Avg. Temp.', 'Percipitation']],typ='levels').rename('SARIMAX Predictions')
pred.index = test.index

last_original_value = df['Volume'].iloc[-31]

pred_reversed = pred.copy()
pred_reversed[0] = last_original_value + pred_reversed[0]
pred_reversed = pred_reversed.cumsum()
pred_reversed.index = test.index

pred_reversed.plot(legend=True,figsize=(10,5))
plt.grid(True)
test['Volume'].plot(legend=True)
plt.grid(True)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

rmse_sarimax = np.sqrt(mean_squared_error(test['Volume'], pred_reversed))
nrmse_sarimax = rmse_sarimax / np.mean(test['Volume']) * 100
mae_sarimax = mean_absolute_error(test['Volume'], pred_reversed)
r2_sarimax = r2_score(test['Volume'], pred_reversed)

print(f"Root Mean Square Error for SARIMAX Model : {rmse_sarimax}")
print(f"Normalized Root Mean Square Error for SARIMAX Model : {nrmse_sarimax} % ")
print(f"Mean Absolute Error for SARIMAX Model: {mae_sarimax}")
print(f"R-squared for SARIMAX Model: {r2_sarimax}")

"""# VAR"""

from statsmodels.tsa.api import VAR

model = VAR(train_diff)
for i in [1,2,3,4,5,6,7,8,9,10]:
    results = model.fit(i)
    print('Order =', i)
    print('AIC: ', results.aic)
    print('BIC: ', results.bic)
    print()

lag_order = results.k_ar
print(lag_order)

z = results.forecast(y=train_diff.values[-lag_order:], steps=30)

results= model.fit(10)
results.summary()

idx = pd.date_range('2024-12-30	', periods=30, freq='D')
df_forecast = pd.DataFrame(z, index=idx, columns=['Volume_1d','Avg. Temp._1d','Percipitation_1d', 'Holiday_1d'])
df_forecast.tail()

nobs = 30

for col in ['Volume', 'Avg. Temp.', 'Percipitation']:
    # Reverse the first difference for the current column
    df_forecast[f'{col}'] = (df[col].iloc[-nobs-1] - df[col].iloc[-nobs-2]) + df_forecast[f'{col}_1d'].cumsum()

    df_forecast[f'{col}Forecast'] = df[col].iloc[-nobs-1] + df_forecast[f'{col}_1d'].cumsum()

df_forecast = df_forecast[['Volume_1d', 'VolumeForecast',
                           'Avg. Temp._1d', 'Avg. Temp.Forecast',
                           'Percipitation_1d', 'PercipitationForecast']]
df_forecast.head()

test['Volume'].plot(figsize=(10,5),legend=True).autoscale(axis='x',tight=True)
plt.grid(True)
df_forecast['VolumeForecast'].plot(legend=True)
plt.grid(True)

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse_var = np.sqrt(mean_squared_error(df['Volume'][-nobs:], df_forecast['VolumeForecast'][-nobs:]))
nrmse_var = rmse_var / np.mean(test['Volume']) * 100
mae_var = mean_absolute_error(df['Volume'][-nobs:], df_forecast['VolumeForecast'][-nobs:])
r2_var = r2_score(df['Volume'][-nobs:], df_forecast['VolumeForecast'][-nobs:])

print(f"Root Mean Square Error for VAR Model : {rmse_var}")
print(f"Normalized Root Mean Square Error for VAR Model : {nrmse_var} %  ")
print(f"Mean Absolute Error for VAR Model: {mae_var}")
print(f"R-squared for VAR Model:  {r2_var}")

"""# VARMA"""

!pip install pmdarima
from pmdarima import auto_arima
from statsmodels.tsa.statespace.varmax import VARMAX, VARMAXResults

#auto_arima(test['Volume'],maxiter=1000)

#auto_arima(test['Avg. Temp.'],maxiter=1000)

model = VARMAX(train_diff, order=(1,1), trend='ct')
results = model.fit(maxiter=1000, disp=False)

results.summary()

df_forecast = results.forecast(40)
df_forecast.tail()

nobs = 30
# Revercing Diffrencing
df_forecast['Volume1D'] = (df['Volume'].iloc[-nobs-1]-df['Volume'].iloc[-nobs-2]) + df_forecast['Volume'].cumsum()
df_forecast['Avg. Temp.1D'] = (df['Avg. Temp.'].iloc[-nobs-1]-df['Avg. Temp.'].iloc[-nobs-2]) + df_forecast['Avg. Temp.'].cumsum()
df_forecast['Percipitation1D'] = (df['Percipitation'].iloc[-nobs-1]-df['Percipitation'].iloc[-nobs-2]) + df_forecast['Percipitation'].cumsum()

# forecast values from the first difference set
df_forecast['VolumeForecast'] = df['Volume'].iloc[-nobs-1] + df_forecast['Volume'].cumsum()
df_forecast['Avg. Temp.Forecast'] = df['Avg. Temp.'].iloc[-nobs-1] + df_forecast['Avg. Temp.'].cumsum()
df_forecast['PercipitationForecast'] = df['Percipitation'].iloc[-nobs-1] + df_forecast['Percipitation'].cumsum()

df_forecast.tail()

start_index = test['Volume'].index[0]
forecast_len = len(test['Volume'])
forecast_data = df_forecast['VolumeForecast'].loc[start_index:].iloc[:forecast_len]

test['Volume'].plot(figsize=(10,5), legend=True).autoscale(axis='x', tight=True)
forecast_data.plot(legend=True)
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

common_index = df.index.intersection(df_forecast.index[-nobs:])

actual_values = df['Volume'].loc[common_index]
forecast_data = df_forecast['VolumeForecast'].loc[common_index]
actual_values = actual_values.dropna()
forecast_data = forecast_data.dropna()

min_len = min(len(actual_values), len(forecast_data))
actual_values = actual_values[:min_len]
forecast_data = forecast_data[:min_len]

rmse_varma = np.sqrt(mean_squared_error(actual_values, forecast_data))
nrmse_varma = rmse_varma / np.mean(actual_values) * 100
mae_varma = mean_absolute_error(actual_values, forecast_data)
r2_varma = r2_score(actual_values, forecast_data)

print(f"Root Mean Square Error for VARMA Model : {rmse_varma}")
print(f"Normalized Root Mean Square Error for VARMA Model : {nrmse_varma} %  ")
print(f"Mean Absolute Error for VARMA Model: {mae_varma}")
print(f"R-squared for VARMA Model:  {r2_varma}")

"""# Prophet
Here we will reintroduce the dataframe beacuse Prophet requires the date and target coloumn to be renamed to 'ds' , 'y' repectively.

# FB Prophet
"""

#!pip install prophet
from prophet import Prophet

df1 = pd.read_csv('data volume.csv')

df1 = df1.rename(columns={'Date': 'ds', 'Volume': 'y'})
df1.tail(20)

model = Prophet(seasonality_mode='multiplicative')

model.add_regressor('Holiday')
model.add_regressor('Max_Temp')
model.add_regressor('Min._Temp.')
model.add_regressor('Avg. Temp.')
model.add_regressor('Percipitation')
model.fit(df1)

future = model.make_future_dataframe(periods=0)
future = pd.concat([future, df1[['Holiday', 'Max_Temp',	'Min._Temp.', 'Avg. Temp.', 'Percipitation']].reset_index(drop=True)], axis=1)
future

forecast = model.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

fig = plt.figure(figsize=(20, 5))
ax = fig.add_subplot()
fig = model.plot(forecast, ax=ax)

plt.show()
plt.show()

df1['y'].plot(figsize=(20,7),legend=True,label='Volume').autoscale(axis='x',tight=True)
plt.plot(forecast["yhat"], color='red',label='Forecast')
plt.grid(True)
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

def adjusted_r2(y_true, y_pred, n_features):
    r2 = r2_score(y_true, y_pred)
    n_samples = len(y_true)
    adjusted_r2 = 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)
    return adjusted_r2

prediction = forecast['yhat'][:-14]

rmse_prophet = np.sqrt(mean_squared_error(df1['y'][:-14], prediction))
nrmse_prophet = rmse_prophet / np.mean(df1['y'][:-14]) * 100
mae_prophet = mean_absolute_error(df1['y'][:-14], prediction)
r2_prophet = r2_score(df1['y'][:-14], prediction)

print(f"Root Mean Square Error for Prophet Model : {rmse_prophet}")
print(f"Normalized Root Mean Square Error for Prophet Model : {nrmse_prophet} %  ")
print(f"Mean Absolute Error for Prophet Model: {mae_prophet}")
print(f"R-squared for Prophet Model:  {r2_prophet}")

"""# FB Prophet 2

"""

#!pip install prophet
from prophet import Prophet

df2 = pd.read_csv('data volume.csv')

df2 = df2.rename(columns={'Date': 'ds', 'Volume': 'y'})
df2.tail(20)

model_2 = Prophet(seasonality_mode='multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=True,
    holidays_prior_scale=15,
    changepoint_prior_scale=0.1,
    mcmc_samples=50)

model_2.add_regressor('Holiday')
model_2.add_regressor('Max_Temp')
model_2.add_regressor('Min._Temp.')
model_2.add_regressor('Avg. Temp.')
model_2.add_regressor('Percipitation')
model_2.fit(df2)

future_2 = model_2.make_future_dataframe(periods=0)
future_2 = pd.concat([future_2, df2[['Holiday','Max_Temp','Min._Temp.', 'Avg. Temp.', 'Percipitation']].reset_index(drop=True)], axis=1)
future_2

forecast_2 = model_2.predict(future_2)
forecast_2[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

fig = model_2.plot(forecast_2)
fig.set_size_inches(20,5)
print()

df2['y'].plot(figsize=(20,5), legend=True, label='Volume').autoscale(axis='x', tight=True)
plt.plot(forecast_2["yhat"], color='red', label='Forecast')
plt.grid(True)
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

prediction_2 = forecast_2['yhat'][:-14]

rmse_prophet2 = np.sqrt(mean_squared_error(df2['y'][:-14], prediction_2))
nrmse_prophet2 = rmse_prophet2 / np.mean(df2['y'][:-14]) * 100
mae_prophet2 = mean_absolute_error(df2['y'][:-14], prediction_2)
r2_prophet2 = r2_score(df2['y'][:-14], prediction_2)

print(f'RMSE for model 2: {rmse_prophet2}')
print(f'NRMSE for model 2: {nrmse_prophet2} %')
print(f'MAE for model 2: {mae_prophet2}')
print(f'R-squared for model 2: {r2_prophet2}')

"""# FB Prophet 3

"""

#!pip install prophet
from prophet import Prophet

df3 = pd.read_csv('data volume.csv', index_col='Date',parse_dates=True)

df3['Year'] = df3.index.year
df3['Month'] = df3.index.month
df3['Day'] = df3.index.day
df3['Quater'] = df3.index.quarter
df3['Day of Week'] = df3.index.dayofweek
df3.info()

df3 = df3.reset_index()
df3 = df3.rename(columns={'Date': 'ds', 'Volume': 'y'})
#df3 = df3.drop(['index','level_0'], axis=1)
df3

model_3 = Prophet(seasonality_mode='multiplicative',
    yearly_seasonality=True,
    weekly_seasonality=True,
    daily_seasonality=True,
    )

model_3.add_regressor('Holiday')
model_3.add_regressor('Avg. Temp.')



model_3.fit(df3)

future_3 = model_3.make_future_dataframe(periods=0)
future_3 = pd.concat([future_3, df3[['Holiday', 'Avg. Temp.',]].reset_index(drop=True)], axis=1)
future_3

forecast_3 = model_3.predict(future_3)
forecast_3[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]

fig = model_3.plot(forecast_3)
fig.set_size_inches(20,5)
print()

df3['y'].plot(figsize=(20,5), legend=True, label='Volume').autoscale(axis='x', tight=True)
plt.plot(forecast_3["yhat"], color='red', label='Forecast')
plt.grid(True)
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

prediction_3 = forecast_3['yhat'][:-14]

rmse_prophet3= np.sqrt(mean_squared_error(df3['y'][:-14], prediction_3))
nrmse_prophet3 = rmse_prophet3 / np.mean(df3['y'][:-14]) * 100
mae_prophet3 = mean_absolute_error(df3['y'][:-14], prediction_3)
r2_prophet3 = r2_score(df3['y'][:-14], prediction_3)

print(f'RMSE for model 3: {rmse_prophet3}')
print(f'NRMSE for model 3: {nrmse_prophet3} %')
print(f'MAE for model 3: {mae_prophet3}')
print(f'R-squared for model 3: {r2_prophet3}')

"""# Deep Learning Models"""

#!pip install keras
#!pip install tensorflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from sklearn.preprocessing import MinMaxScaler

#from google.colab import drive
#drive.mount('/content/drive')

df4= pd.read_csv('data volume.csv')
df4 = df4[:-14]

df4['Date'] = pd.to_datetime(df4['Date'], format='%d-%b-%y',)
df4 = df4.sort_values(by='Date')

features = ['Holiday', 'Max_Temp', 'Min._Temp.', 'Avg. Temp.', 'Percipitation']
target = 'Volume'

"""LSTMs often use activation functions like sigmoid or tanh, which have limited output ranges. Scaling the data to match these ranges helps prevent saturation and allows the model to learn more effectively."""

scaler_X = MinMaxScaler()
scaler_y = MinMaxScaler()
df4[features] = scaler_X.fit_transform(df4[features])
df4[target] = scaler_y.fit_transform(df4[[target]])

"""Creating input sequences and corresponding target values for the LSTM model using a sliding window approach.

"""

sequence_length = 10
X, y = [], []
for i in range(len(df4) - sequence_length):
    X.append(df4[features].iloc[i:i+sequence_length].values)
    y.append(df4[target].iloc[i + sequence_length])

X, y = np.array(X), np.array(y)

train_size = int(0.8 * len(X))
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
len(X_train), len(X_test), len(y_train), len(y_test)

""" # Long Short-Term Memoy (LSTM) based Recurrent Neural Networks (RNN)"""

from tensorflow.keras.layers import LSTM, Dense, Dropout

"""RNNs are designed to handle sequential data by maintaining a "hidden state" that captures information from previous time steps. They process input sequences one step at a time, updating the hidden state at each step."""

model1 = Sequential([
    LSTM(64, activation='relu', return_sequences=True, input_shape=(sequence_length, len(features))),
    Dropout(0.2),
    LSTM(32, activation='relu', return_sequences=False),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1, activation='linear')
])
model1.summary()

model1.compile(optimizer='adam', loss='mse')

history1 = model1.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

plt.plot(history1.history['loss'], label='Train Loss')
plt.plot(history1.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title("LSTM Model Training Loss")
plt.show()

y_pred1 = model1.predict(X_test)

y_pred_rescaled1 = scaler_y.inverse_transform(y_pred1)
y_test_rescaled1 = scaler_y.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(20,5))
plt.plot(y_test_rescaled1, label="Actual Volume", color="blue")
plt.plot(y_pred_rescaled1, label="RNN Predicted Volume", color="red")
plt.xlabel("Time Steps")
plt.ylabel("Volume")
plt.title("LSTM Model: Actual vs Predicted Volume")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse_lstm = np.sqrt(mean_squared_error(y_pred_rescaled1, y_test_rescaled1))
nrmse_lstm =  rmse_lstm / np.mean(y_test_rescaled1) * 100
mae_lstm = mean_absolute_error(y_pred_rescaled1, y_test_rescaled1)
r2_lstm = r2_score(y_pred_rescaled1, y_test_rescaled1)


print(f"Root Mean Square Error for RNN Model : {rmse_lstm}")
print(f"Normalized Root Mean Square Error for RNN Model : {nrmse_lstm} %  ")
print(f"Mean Absolute Error for RNN Model: {mae_lstm}")
print(f"R-squared for RNN Model:  {r2_lstm}")

"""# Convolutional Neural Network (CNN)

"""

from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten

""" CNNs use convolutional filters to extract local patterns from data. For time series, 1D convolutions are applied across the time dimension to capture temporal features."""

model2 = Sequential([
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(sequence_length, len(features))),MaxPooling1D(pool_size=2),Flatten(),
    Dense(32, activation='relu'),
    Dense(1,  activation='linear')
])
model2.summary()

model2.compile(optimizer='adam', loss='mse')

history2 = model2.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

plt.plot(history2.history['loss'], label='Train Loss')
plt.plot(history2.history['val_loss'], label='Validation Loss')
plt.title('CNN Model Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True)
plt.show()

y_pred2 = model2.predict(X_test)

y_pred_rescaled2 = scaler_y.inverse_transform(y_pred2)
y_test_rescaled2 = scaler_y.inverse_transform(y_test.reshape(-1, 1))

plt.figure(figsize=(20,5))
plt.plot(y_test_rescaled2, label="Actual Volume")
plt.plot(y_pred_rescaled2, label="CNN Predicted Volume")
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse_cnn = np.sqrt(mean_squared_error(y_pred_rescaled2, y_test_rescaled2))
nrmse_cnn = rmse_cnn / np.mean(y_test_rescaled2) * 100
mae_cnn = mean_absolute_error(y_pred_rescaled2, y_test_rescaled2)
r2_cnn = r2_score(y_pred_rescaled2, y_test_rescaled2)

print(f"Root Mean Square Error for CNN Model : {rmse_cnn}")
print(f"Normalized Root Mean Square Error for CNN Model : {nrmse_cnn} %  ")
print(f"Mean Absolute Error for CNN Model: {mae_cnn}")
print(f"R-squared for CNN Model:  {r2_cnn}")

"""# Gated Recurrent Unit (GRU)"""

from tensorflow.keras.layers import GRU

"""GRUs are a variant of RNNs designed to address the vanishing gradient problem. They use gating mechanisms (update and reset gates) to control the flow of information, allowing them to capture long-term dependencies more effectively."""

model3 = Sequential([
    GRU(64, activation='relu', return_sequences=True, input_shape=(sequence_length, len(features))),Dropout(0.2),
    GRU(32, activation='relu', return_sequences=False),Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1, activation='linear')
])
model3.summary()

model3.compile(optimizer='adam', loss='mse')

history3 = model3.fit(X_train, y_train, epochs=20, batch_size=30, validation_data=(X_test, y_test))

plt.plot(history3.history['loss'], label='Train Loss')
plt.plot(history3.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.title("GRU Model Training Loss")
plt.show()

y_pred3 = model3.predict(X_test)

y_pred_rescaled3 = scaler_y.inverse_transform(y_pred3)
y_test_rescaled3 = scaler_y.inverse_transform(y_test.reshape(-1, 1))

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

rmse_gru = np.sqrt(mean_squared_error(y_pred_rescaled3, y_test_rescaled3))
nrmse_gru = rmse_gru / np.mean(y_test_rescaled3) * 100
mae_gru = mean_absolute_error(y_pred_rescaled3, y_test_rescaled3)
r2_gru = r2_score(y_pred_rescaled3, y_test_rescaled3)

print(f"Root Mean Square Error for GRU Model : {rmse_gru}")
print(f"Normalized Root Mean Square Error for GRU Model : {nrmse_gru} %  ")
print(f"Mean Absolute Error for GRU Model: {mae_gru}")
print(f"R-squared for GRU Model:  {r2_gru}")

plt.figure(figsize=(20,5))
plt.plot(y_test_rescaled3, label="Actual Volume")
plt.plot(y_pred_rescaled3, label="Predicted Volume")
plt.legend()
plt.show()

"""# Final Observation"""

models = pd.DataFrame({
    'Model': ['Seasonal Autoregressive Integrated Moving Average with Exogenous', 'Vector Auto Regression', 'Vector Auto Regression Moving Average', 'Prophet', 'Prophet 2','Prophet 3' , 'Recurrent Neural Network', 'Convolution Neural Network', 'Gated recurrent unit'],
    'Root Mean Square Error': [rmse_sarimax, rmse_var, rmse_varma, rmse_prophet, rmse_prophet2,rmse_prophet3, rmse_lstm, rmse_cnn, rmse_gru],
    'Normalized Root Mean Square Error': [nrmse_sarimax, nrmse_var, nrmse_varma, nrmse_prophet, nrmse_prophet2,nrmse_prophet3, nrmse_lstm, nrmse_cnn, nrmse_gru],
    'Mean Absolute Error': [mae_sarimax, mae_var, mae_varma, mae_prophet, mae_prophet2, mae_prophet3, mae_lstm, mae_cnn, mae_gru],
    'R-Squared': [r2_sarimax, r2_var, r2_varma, r2_prophet, r2_prophet2, r2_prophet3, r2_lstm, r2_cnn, r2_gru]
    })

models['Normalized Root Mean Square Error'] = models['Normalized Root Mean Square Error'].apply(lambda x: f'{x:.2f}%')

models = models.sort_values(by='Normalized Root Mean Square Error', ascending=True).style.hide(axis="index").set_properties(**{'font-size': '14pt'})
display(models)

"""*   Root mean squared error (RMSE) is the square root of the mean of the square of all of the error
*   Normalized Root Mean Square Error (NRMSE) measures the accuracy of a predictive model by comparing the predicted values to the observed values
*   Mean Absolute Error (MAE) is a metric that calculates the average magnitude of the absolute errors between the predicted and actual values.
*   The R-squared metric (R^{2}) is a statistical measure that shows how well a regression model fits the data

# **The Prophet  model demonstrates the best overall performance among the evaluated models, achieving the lowest RMSE ,NRMSE percentage and MAE values, while having a highest R-squared value. This suggests that Prophet provides the most accurate predictions with the least error.**

# Business Insights & Implementation
The insights derived from this forecasting model can be effectively utilized by curd manufacturers and distributors to optimize their operations, minimize wastage, and improve profitability. Below is key business recommendations based on the analysis:

#1. Demand-Based Production Planning

•	Insight: Forecasting allows manufacturers to anticipate spikes in demand, especially during certain seasons, holidays, or temperature changes.

•	Actionable Strategy:

o	Adjust production schedules to increase supply during high-demand periods (e.g., summer months, festive seasons) and scale down during low-demand periods to prevent overproduction and wastage.

o	Utilize historical data trends and predicted demand to manage raw material procurement efficiently (e.g., ensuring milk supply aligns with expected curd production).

#2. Inventory & Supply Chain Optimization

•	Insight: Poor demand forecasting can lead to excessive inventory, leading to spoilage and financial losses.

•	Actionable Strategy:

o	Implement a just-in-time inventory approach using daily or weekly demand forecasts to prevent overstocking.

o	Work closely with retailers and distributors to align stock levels with expected demand and avoid unnecessary surplus.

o	Use real-time forecasting models to update production levels dynamically based on changes in consumer demand patterns.

#3. Regional Distribution & Expansion

•	Insight: Demand varies by region based on weather, festivals, and consumption habits.
•	Actionable Strategy:

o	Identify regions with higher demand and focus distribution efforts accordingly.
       
o	Use predictive insights to expand into new markets with high curd consumption potential.

o	Optimize logistics routes to ensure faster delivery to high-demand areas, reducing spoilage risks.

# Final Thoughts
By leveraging the demand forecasting model, fast-moving consumer goods (FMCG)
sector can shift from reactive to proactive decision-making, reducing waste, optimizing supply chains, and maximizing revenue. Implementing these strategies ensures sustained growth and better market competitiveness.
"""